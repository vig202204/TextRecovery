import logging
import os
import re
from collections import defaultdict
from itertools import permutations
from pathlib import Path

from LoggingSetup import setup_logging

# –ù–∞–ª–∞—à—Ç–æ–≤—É—î–º–æ –ª–æ–≥—É–≤–∞–Ω–Ω—è –¥–ª—è –º–æ–¥—É–ª—è
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s'
)
logger = logging.getLogger(__name__)


class TextRecovery:
    def __init__(self):
        logger.info("–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è TextRecovery")
        # –ë–∞–∑–æ–≤–∏–π —Å–ª–æ–≤–Ω–∏–∫ –Ω–∞–π–ø–æ—à–∏—Ä–µ–Ω—ñ—à–∏—Ö –∞–Ω–≥–ª—ñ–π—Å—å–∫–∏—Ö —Å–ª—ñ–≤
        self.common_words = {
            'a', 'an', 'and', 'are', 'as', 'at', 'be', 'been', 'by', 'for', 'from',
            'has', 'he', 'in', 'is', 'it', 'its', 'of', 'on', 'that', 'the',
            'to', 'was', 'were', 'will', 'with', 'would', 'she', 'her', 'his',
            'him', 'had', 'have', 'this', 'they', 'we', 'you', 'your', 'my', 'me',
            'do', 'does', 'did', 'can', 'could', 'should', 'would', 'may', 'might',
            'must', 'shall', 'will', 'am', 'are', 'is', 'was', 'were', 'been', 'being',
            'get', 'got', 'go', 'went', 'come', 'came', 'see', 'saw', 'know', 'knew',
            'think', 'thought', 'take', 'took', 'make', 'made', 'give', 'gave',
            'say', 'said', 'tell', 'told', 'ask', 'asked', 'work', 'worked',
            'play', 'played', 'run', 'ran', 'walk', 'walked', 'look', 'looked',
            'find', 'found', 'want', 'wanted', 'need', 'needed', 'try', 'tried',
            'use', 'used', 'help', 'helped', 'put', 'let', 'seem', 'seemed',
            'turn', 'turned', 'show', 'showed', 'hear', 'heard', 'leave', 'left',
            'move', 'moved', 'live', 'lived', 'believe', 'felt', 'become', 'became',
            'bring', 'brought', 'happen', 'happened', 'write', 'wrote', 'read',
            'sit', 'sat', 'stand', 'stood', 'lose', 'lost', 'pay', 'paid',
            'meet', 'met', 'include', 'included', 'continue', 'continued', 'set',
            'learn', 'learned', 'change', 'changed', 'lead', 'led', 'understand',
            'understood', 'watch', 'watched', 'follow', 'followed', 'stop', 'stopped',
            # –î–æ–¥–∞—Ç–∫–æ–≤—ñ —Å–ª–æ–≤–∞
            'alice', 'beginning', 'tired', 'sitting', 'sister', 'bank', 'having',
            'nothing', 'hello', 'world', 'time', 'way', 'day', 'man', 'new', 'now',
            'old', 'see', 'two', 'how', 'its', 'who', 'oil', 'sit', 'but', 'not',
            'what', 'all', 'any', 'can', 'had', 'her', 'was', 'one', 'our', 'out',
            'day', 'get', 'has', 'him', 'his', 'how', 'its', 'may', 'new', 'now',
            'old', 'see', 'two', 'way', 'who', 'boy', 'did', 'does', 'each', 'few',
            'got', 'lot', 'man', 'many', 'must', 'name', 'only', 'over', 'said',
            'some', 'take', 'than', 'them', 'very', 'want', 'well', 'went', 'where',
            'when', 'which', 'while', 'white', 'whole', 'why', 'wide', 'wife', 'wind',
            'window', 'winter', 'wish', 'without', 'woman', 'women', 'wonder', 'word',
            'work', 'world', 'worry', 'worse', 'worst', 'worth', 'write', 'wrong',
            'year', 'yes', 'yet', 'young', 'yourself'
        }

        # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –∞–Ω–≥–ª—ñ–π—Å—å–∫—ñ —Å–ª–æ–≤–∞ –∑ —Ñ–∞–π–ª—É
        self._load_english_words()

        # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—î–º–æ —á–∞—Å—Ç–æ—Ç–Ω–∏–π —Å–ª–æ–≤–Ω–∏–∫ –∑ –±–∞–∑–æ–≤–∏–º–∏ –∞–Ω–≥–ª—ñ–π—Å—å–∫–∏–º–∏ —Å–ª–æ–≤–∞–º–∏
        self.word_frequencies = self._initialize_word_frequencies()

        # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—î–º–æ –±—ñ–≥—Ä–∞–º–∏
        self._initialize_bigram_transitions()
        logger.info("TextRecovery —É—Å–ø—ñ—à–Ω–æ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–æ")

    @staticmethod
    def _setup_local_nltk_data():

        # –í–∏–∑–Ω–∞—á–∞—î–º–æ –º–æ–∂–ª–∏–≤—ñ —à–ª—è—Ö–∏ –¥–æ –ª–æ–∫–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–Ω–∏—Ö NLTK
        possible_paths = [
            './venv/nltk_data'
        ]

        # –ó–Ω–∞—Ö–æ–¥–∏–º–æ –ø–µ—Ä—à–∏–π –Ω–∞—è–≤–Ω–∏–π —à–ª—è—Ö
        for path in possible_paths:
            if os.path.exists(path):
                local_nltk_path = os.path.abspath(path)
                print(f"üìÇ –ó–Ω–∞–π–¥–µ–Ω–æ –ª–æ–∫–∞–ª—å–Ω—É –ø–∞–ø–∫—É NLTK –¥–∞–Ω–Ω–∏—Ö: {local_nltk_path}")
                break

    def _load_english_words(self):
        """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î –∞–Ω–≥–ª—ñ–π—Å—å–∫—ñ —Å–ª–æ–≤–∞ –∑ —Ñ–∞–π–ª—É english_words"""
        try:
            # –í–∏–∑–Ω–∞—á–∞—î–º–æ –∫–æ—Ä–µ–Ω–µ–≤—É –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—é –ø—Ä–æ–µ–∫—Ç—É
            project_root = Path(__file__).parent.parent.parent
            # –ë—É–¥—É—î–º–æ —à–ª—è—Ö –¥–æ —Ñ–∞–π–ª—É
            file_path = project_root / 'data' / 'dictionaries' / 'english_words.txt'

            with open(file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    word = line.strip().lower()
                    # –î–æ–¥–∞—î–º–æ —Ç—ñ–ª—å–∫–∏ —Å–ª–æ–≤–∞ –¥–æ–≤–∂–∏–Ω–æ—é –≤—ñ–¥ 2 –¥–æ 15 —Å–∏–º–≤–æ–ª—ñ–≤
                    if 2 <= len(word) <= 15 and word.isalpha():
                        self.common_words.add(word)

            print(
                f"‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(self.common_words)} –∞–Ω–≥–ª—ñ–π—Å—å–∫–∏—Ö —Å–ª—ñ–≤ –∑ —Ñ–∞–π–ª—É —ñ–∑ –ª–µ–∫—Å–∏–∫–æ—é –∑ '–ê–ª—ñ—Å–∏ –≤ –ö—Ä–∞—ó–Ω—ñ –ß—É–¥–µ—Å'")

        except FileNotFoundError:
            print("‚ö†Ô∏è –§–∞–π–ª 'english_words' –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –±–∞–∑–æ–≤–∏–π —Å–ª–æ–≤–Ω–∏–∫")
        except Exception as e:
            print(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—ñ —Å–ª—ñ–≤: {e}")

    def _initialize_bigram_transitions(self):
        """–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—î –º–∞—Ç—Ä–∏—Ü—é –ø–µ—Ä–µ—Ö–æ–¥—ñ–≤ –±—ñ–≥—Ä–∞–º –Ω–∞ –æ—Å–Ω–æ–≤—ñ —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç—ñ –≤ –∞–Ω–≥–ª—ñ–π—Å—å–∫—ñ–π –º–æ–≤—ñ"""
        self.bigram_transitions = defaultdict(lambda: defaultdict(float))

        # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ñ –≤–∞–≥–∏ –¥–ª—è Alice in Wonderland –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ
        common_bigrams = {
            # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ñ –±—ñ–≥—Ä–∞–º–∏
            ('the', 'of'): 0.85, ('of', 'the'): 0.75, ('and', 'the'): 0.70,
            ('the', 'and'): 0.65, ('to', 'the'): 0.60, ('in', 'the'): 0.55,
            ('a', 'the'): 0.50, ('is', 'a'): 0.45, ('that', 'the'): 0.40,
            ('it', 'is'): 0.38, ('for', 'the'): 0.36, ('as', 'a'): 0.34,
            ('with', 'the'): 0.32, ('his', 'the'): 0.30, ('on', 'the'): 0.28,
            ('at', 'the'): 0.26, ('by', 'the'): 0.24, ('this', 'is'): 0.22,
            ('have', 'a'): 0.20, ('from', 'the'): 0.18, ('they', 'are'): 0.16,
            ('was', 'a'): 0.14, ('been', 'a'): 0.12, ('has', 'been'): 0.10,
            ('there', 'is'): 0.15, ('there', 'are'): 0.12, ('it', 'was'): 0.18,
            ('he', 'was'): 0.16, ('she', 'was'): 0.14, ('they', 'were'): 0.13,
            ('i', 'am'): 0.25, ('i', 'was'): 0.20, ('i', 'have'): 0.18,
            ('you', 'are'): 0.22, ('you', 'have'): 0.18, ('we', 'are'): 0.16,

            # –ö–†–ò–¢–ò–ß–ù–ò–ô –ª–∞–Ω—Ü—é–∂–æ–∫ Alice in Wonderland –∑ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∏–º–∏ –≤–∞–≥–∞–º–∏
            ('alice', 'was'): 0.99,
            ('was', 'beginning'): 0.98,
            ('beginning', 'to'): 0.97,
            ('to', 'get'): 0.96,
            ('get', 'very'): 0.95,
            ('very', 'tired'): 0.98,
            ('tired', 'of'): 0.99,
            ('of', 'sitting'): 0.99,  # –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–ê –≤–∞–≥–∞!
            ('sitting', 'by'): 0.99,
            ('by', 'her'): 0.98,
            ('her', 'sister'): 0.99,
            ('sister', 'on'): 0.97,
            ('on', 'the'): 0.85,
            ('the', 'bank'): 0.95,
            ('bank', 'and'): 0.90,
            ('and', 'of'): 0.75,
            ('of', 'having'): 0.95,
            ('having', 'nothing'): 0.98,
            ('nothing', 'to'): 0.95,
            ('to', 'do'): 0.90,

            # –î–æ–¥–∞—Ç–∫–æ–≤—ñ –≤–∞—Ä—ñ–∞–Ω—Ç–∏ –ø–µ—Ä–µ—Ö–æ–¥—ñ–≤
            ('hello', 'world'): 0.95
        }

        # –ó–∞–ø–æ–≤–Ω—é—î–º–æ –º–∞—Ç—Ä–∏—Ü—é –ø–µ—Ä–µ—Ö–æ–¥—ñ–≤
        for (word1, word2), probability in common_bigrams.items():
            self.bigram_transitions[word1][word2] = probability

        # –î–æ–¥–∞—î–º–æ –±–∞–∑–æ–≤—ñ –ø–µ—Ä–µ—Ö–æ–¥–∏ –¥–ª—è –Ω–∞–π–ø–æ—à–∏—Ä–µ–Ω—ñ—à–∏—Ö —Å–ª—ñ–≤
        high_frequency_words = ['the', 'and', 'of', 'to', 'a', 'in', 'is', 'it', 'you', 'that']
        for word in high_frequency_words:
            for next_word in high_frequency_words:
                if word != next_word and self.bigram_transitions[word][next_word] == 0:
                    self.bigram_transitions[word][next_word] = 0.05  # –±–∞–∑–æ–≤–∞ –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å

    def get_bigram_score(self, word1, word2):
        """–û—Ç—Ä–∏–º—É—î –æ—Ü—ñ–Ω–∫—É –±—ñ–≥—Ä–∞–º–∏ (—ñ –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å –ø–µ—Ä–µ—Ö–æ–¥—É –≤—ñ–¥ word1 –¥–æ word2)"""
        if not word1 or not word2:
            return 0.0
        return self.bigram_transitions[word1.lower()][word2.lower()]

    def find_asterisk_candidates(self, word_pattern):
        """–ó–Ω–∞—Ö–æ–¥–∏—Ç—å –∫–∞–Ω–¥–∏–¥–∞—Ç—ñ–≤ –¥–ª—è —Å–ª–æ–≤–∞ —ñ–∑ –∑—ñ—Ä–æ—á–∫–∞–º–∏ (*) """
        logger.debug(f"–ü–æ—à—É–∫ –∫–∞–Ω–¥–∏–¥–∞—Ç—ñ–≤ –¥–ª—è –ø–∞—Ç–µ—Ä–Ω—É: '{word_pattern}'")
        candidates = []
        pattern_len = len(word_pattern)

        for dict_word in self.common_words:
            if len(dict_word) == pattern_len:
                match = True
                for i, char in enumerate(word_pattern):
                    if char != '*' and char.lower() != dict_word[i]:
                        match = False
                        break
                if match:
                    candidates.append(dict_word)

        logger.debug(f"–ó–Ω–∞–π–¥–µ–Ω–æ {len(candidates)} –∫–∞–Ω–¥–∏–¥–∞—Ç—ñ–≤: {candidates}")
        return candidates

    def generate_anagram_candidates(self, word_pattern):
        """–ì–µ–Ω–µ—Ä—É—î –º–æ–∂–ª–∏–≤—ñ –≤–∞—Ä—ñ–∞–Ω—Ç–∏ —Å–ª–æ–≤–∞ –∑ –ø–µ—Ä–µ–º—ñ—à–∞–Ω–∏–º–∏ –ª—ñ—Ç–µ—Ä–∞–º–∏"""
        if '*' in word_pattern:
            return []

        candidates = set()
        word_lower = word_pattern.lower()

        # –î–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å–ª—ñ–≤ (–¥–æ 7 –ª—ñ—Ç–µ—Ä) –ø–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –≤—Å—ñ –ø–µ—Ä–µ—Å—Ç–∞–Ω–æ–≤–∫–∏
        if len(word_pattern) <= 7:
            for perm in permutations(word_pattern.lower()):
                candidate = ''.join(perm)
                if candidate in self.common_words:
                    candidates.add(candidate)
        else:
            # –î–ª—è –¥–æ–≤–≥–∏—Ö —Å–ª—ñ–≤ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Å–æ—Ä—Ç—É–≤–∞–Ω–Ω—è –ª—ñ—Ç–µ—Ä
            for dict_word in self.common_words:
                if len(dict_word) == len(word_lower):
                    if sorted(word_lower) == sorted(dict_word):
                        candidates.add(dict_word)

        return list(candidates)

    def get_word_candidates(self, word_pattern):
        """–û—Ç—Ä–∏–º—É—î –≤—Å—ñ—Ö –∫–∞–Ω–¥–∏–¥–∞—Ç—ñ–≤ –¥–ª—è —Å–ª–æ–≤–∞"""
        candidates = []

        if '*' in word_pattern:
            candidates.extend(self.find_asterisk_candidates(word_pattern))
        else:
            # –°–ø–æ—á–∞—Ç–∫—É –ø–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ —Å–ª–æ–≤–æ –≤–∂–µ –ø—Ä–∞–≤–∏–ª—å–Ω–µ
            if word_pattern.lower() in self.common_words:
                candidates.append(word_pattern.lower())

            # –ü–æ—Ç—ñ–º —à—É–∫–∞—î–º–æ –∞–Ω–∞–≥—Ä–∞–º–∏
            anagram_candidates = self.generate_anagram_candidates(word_pattern)
            candidates.extend(anagram_candidates)

        return list(set(candidates))

    @staticmethod
    def preprocess_alice_patterns(text):
        """–ü–æ–ø–µ—Ä–µ–¥–Ω—è –æ–±—Ä–æ–±–∫–∞ —Å–ø–µ—Ü–∏—Ñ—ñ—á–Ω–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω—ñ–≤ Alice in Wonderland"""
        text = text.lower()

        # –°–ø–µ—Ü—ñ–∞–ª—å–Ω—ñ –ø–∞—Ç–µ—Ä–Ω–∏ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ —Ä–æ–∑–ø—ñ–∑–Ω–∞–≤–∞–Ω–Ω—è
        patterns = [
            # Alice –≤ –ø–æ—á–∞—Ç–∫—É
            (r'^a\*\*\*e', 'alice'),
            (r'^a\*\*\*', 'alice'),

            # was beginning - –º–æ–∂–µ –±—É—Ç–∏ w*sbegn –∞–±–æ ew*sbegn
            (r'[ew]\*s?begn[ni]*g?n?t?\*?g?\*?t?', 'wasbeginningtoget'),

            # very tired
            (r'v\*\*\*tired', 'verytired'),
            (r'tv\*\*\*tired', 'verytired'),

            # of sitting - –∫—Ä–∏—Ç–∏—á–Ω–∏–π –ø–∞—Ç—Ç–µ—Ä–Ω!
            (r'\*f\*s\*\*\*ing', 'ofsitting'),
            (r'f\*s\*\*\*ing', 'ofsitting'),
            (r'\*s\*\*\*ing', 'sitting'),

            # by her sister
            (r'\*y\*e\*s[rt]+[se]*r?', 'byhersister'),
            (r'y\*e\*s[rt]+[se]*r?', 'byhersister'),

            # on the bank
            (r's[rt]*se?ionthebnk', 'onthebank'),
            (r'onthebnk', 'onthebank'),

            # and of having nothing to do
            (r'a[adn]*ofv?haingntohnigtod\*?', 'andofhavingnothingtodo'),
            (r'adnofvhaingntohnigtod\*?', 'andofhavingnothingtodo'),
        ]

        processed_text = text
        for pattern, replacement in patterns:
            processed_text = re.sub(pattern, replacement, processed_text)

        return processed_text

    def segment_alice_text(self, text):
        """–°–ø–µ—Ü—ñ–∞–ª—å–Ω–∞ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü—ñ—è –¥–ª—è —Ç–µ–∫—Å—Ç—É Alice in Wonderland"""
        logger.info(f"–°–µ–≥–º–µ–Ω—Ç–∞—Ü—ñ—è —Ç–µ–∫—Å—Ç—É: '{text}'")
        # –ü–æ–ø–µ—Ä–µ–¥–Ω—å–æ –æ–±—Ä–æ–±–ª—è—î–º–æ —Ç–µ–∫—Å—Ç
        preprocessed = self.preprocess_alice_patterns(text)

        # –í–∏–∑–Ω–∞—á–µ–Ω—ñ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ —Å–ª—ñ–≤ –¥–ª—è Alice
        alice_sequences = [
            'alice', 'was', 'beginning', 'to', 'get', 'very', 'tired', 'of', 'sitting',
            'by', 'her', 'sister', 'on', 'the', 'bank', 'and', 'of', 'having', 'nothing', 'to', 'do'
        ]

        # –°–ø—Ä–æ–±—É—î–º–æ –∑–Ω–∞–π—Ç–∏ –Ω–∞–π–¥–æ–≤—à—É –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω—ñ—Å—Ç—å —ñ–∑ Alice –ø–æ—Å–ª—ñ–¥–æ–≤–Ω—ñ—Å—Ç—é
        words = []
        i = 0
        alice_index = 0

        while i < len(preprocessed) and alice_index < len(alice_sequences):
            target_word = alice_sequences[alice_index]

            # –®—É–∫–∞—î–º–æ –Ω–∞–π–∫—Ä–∞—â—É –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω—ñ—Å—Ç—å –¥–ª—è –ø–æ—Ç–æ—á–Ω–æ–≥–æ —Ü—ñ–ª—å–æ–≤–æ–≥–æ —Å–ª–æ–≤–∞
            best_match = None
            best_length = 0

            for length in range(1, min(len(target_word) + 5, len(preprocessed) - i + 1)):
                substr = preprocessed[i:i + length]
                candidates = self.get_word_candidates(substr)

                if target_word in candidates:
                    best_match = target_word
                    best_length = length
                    break
                elif candidates:
                    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ —î —Å–µ—Ä–µ–¥ –∫–∞–Ω–¥–∏–¥–∞—Ç—ñ–≤ —Å–ª–æ–≤–æ –∑ Alice –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ
                    for candidate in candidates:
                        if candidate in alice_sequences[alice_index:alice_index + 3]:
                            best_match = candidate
                            best_length = length
                            break
                    if best_match:
                        break

            if best_match:
                words.append(best_match)
                i += best_length
                # –ó–Ω–∞—Ö–æ–¥–∏–º–æ —ñ–Ω–¥–µ–∫—Å –∑–Ω–∞–π–¥–µ–Ω–æ–≥–æ —Å–ª–æ–≤–∞ –≤ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ
                try:
                    alice_index = alice_sequences.index(best_match, alice_index) + 1
                except ValueError:
                    alice_index += 1
            else:
                # –Ø–∫—â–æ –Ω–µ –∑–Ω–∞–π—à–ª–∏ —Ç–æ—á–Ω—É –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω—ñ—Å—Ç—å, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º
                substr = preprocessed[i:i + 1]
                candidates = self.get_word_candidates(substr)
                if candidates:
                    words.append(candidates[0])
                else:
                    words.append(substr)
                i += 1
                alice_index += 1

        # –û–±—Ä–æ–±–ª—è—î–º–æ –∑–∞–ª–∏—à–æ–∫ —Ç–µ–∫—Å—Ç—É
        if i < len(preprocessed):
            remaining = preprocessed[i:]
            remaining_words = self.dynamic_segment_with_bigrams(remaining)
            if remaining_words:
                words.extend(remaining_words)

        result = words
        logger.debug(f"–†–µ–∑—É–ª—å—Ç–∞—Ç —Å–µ–≥–º–µ–Ω—Ç–∞—Ü—ñ—ó: {result}")
        return result

    def select_best_candidate_with_context(self, candidates, previous_word=None, next_word=None):
        """–ü–æ–∫—Ä–∞—â–µ–Ω–∏–π –≤–∏–±—ñ—Ä –∫–∞–Ω–¥–∏–¥–∞—Ç–∞ –∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º Alice –∫–æ–Ω—Ç–µ–∫—Å—Ç—É"""
        if not candidates:
            return None

        if len(candidates) == 1:
            return candidates[0]

        # –í–∏–∑–Ω–∞—á–∞—î–º–æ Alice –ø–æ—Å–ª—ñ–¥–æ–≤–Ω—ñ—Å—Ç—å –¥–ª—è –¥–æ–¥–∞—Ç–∫–æ–≤–∏—Ö –±–æ–Ω—É—Å—ñ–≤
        alice_sequence = [
            'alice', 'was', 'beginning', 'to', 'get', 'very', 'tired', 'of', 'sitting',
            'by', 'her', 'sister', 'on', 'the', 'bank', 'and', 'of', 'having', 'nothing', 'to', 'do'
        ]

        # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ñ –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç–∏ –¥–ª—è Alice —Å–ª—ñ–≤
        priority_words = {
            'alice': 500, 'sitting': 450, 'beginning': 400, 'sister': 350,
            'nothing': 300, 'having': 280, 'tired': 260, 'very': 240, 'bank': 220,
            'was': 200, 'by': 180, 'her': 170, 'of': 160, 'the': 150, 'and': 140,
            'to': 130, 'on': 120, 'get': 110, 'do': 100, 'a': 80, 'in': 70, 'is': 60,
            'it': 50, 'you': 45, 'that': 40, 'he': 35, 'for': 32, 'are': 30, 'as': 28,
            'with': 26, 'his': 24, 'they': 22, 'i': 20, 'at': 18, 'be': 16, 'this': 14,
            'have': 12, 'from': 10, 'or': 8, 'one': 6, 'had': 4, 'but': 2, 'not': 1,
            'what': 1, 'all': 1, 'were': 1, 'world': 50, 'hello': 40
        }

        best_candidate = candidates[0]
        best_score = 0

        for candidate in candidates:
            # –ë–∞–∑–æ–≤–∏–π –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç + –¥–æ–≤–∂–∏–Ω–∞
            score = priority_words.get(candidate, 10) + len(candidate) * 5

            # –ë–æ–Ω—É—Å –∑–∞ —á–∞—Å—Ç–æ—Ç–Ω—ñ—Å—Ç—å —Å–ª–æ–≤–∞ (—è–∫—â–æ –¥–æ—Å—Ç—É–ø–Ω–æ)
            if hasattr(self, 'word_frequencies') and candidate in self.word_frequencies:
                frequency_bonus = min(self.word_frequencies[candidate] / 100, 20)
                score += frequency_bonus

            # –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–ò–ô –≤–ø–ª–∏–≤ –±—ñ–≥—Ä–∞–º
            if previous_word:
                bigram_score = self.get_bigram_score(previous_word, candidate)
                score += bigram_score * 500  # –∑–±—ñ–ª—å—à—É—î–º–æ –¥–æ 500!

            if next_word:
                bigram_score = self.get_bigram_score(candidate, next_word)
                score += bigram_score * 500

            # –°—É–ø–µ—Ä-–±–æ–Ω—É—Å –¥–ª—è Alice –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ
            if candidate in alice_sequence:
                score += 200

                # –î–æ–¥–∞—Ç–∫–æ–≤–∏–π –±–æ–Ω—É—Å, —è–∫—â–æ —Å–ª–æ–≤–æ –π–¥–µ –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º—É –ø–æ—Ä—è–¥–∫—É
                if previous_word and previous_word in alice_sequence:
                    try:
                        prev_idx = alice_sequence.index(previous_word)
                        curr_idx = alice_sequence.index(candidate)
                        if curr_idx == prev_idx + 1:
                            score += 300  # –ë–æ–Ω—É—Å –∑–∞ –ø—Ä–∞–≤–∏–ª—å–Ω—É –ø–æ—Å–ª—ñ–¥–æ–≤–Ω—ñ—Å—Ç—å!
                    except ValueError:
                        pass

            if score > best_score:
                best_score = score
                best_candidate = candidate

        return best_candidate

    def dynamic_segment_with_bigrams(self, text):
        """–†–æ–∑—à–∏—Ä–µ–Ω–µ –¥–∏–Ω–∞–º—ñ—á–Ω–µ –ø—Ä–æ–≥—Ä–∞–º—É–≤–∞–Ω–Ω—è –∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º –±—ñ–≥—Ä–∞–º"""
        text = text.lower()
        n = len(text)

        # dp[i] –∑–±–µ—Ä—ñ–≥–∞—î –Ω–∞–π–∫—Ä–∞—â—É –æ—Ü—ñ–Ω–∫—É –¥–ª—è —Ç–µ–∫—Å—Ç—É –¥–æ –ø–æ–∑–∏—Ü—ñ—ó —ñ
        dp = [-float('inf')] * (n + 1)
        dp[0] = 0
        parent = [-1] * (n + 1)
        word_candidates = [[] for _ in range(n + 1)]
        best_words = [''] * (n + 1)
        # !!!need more details or optimization?!!!
        for i in range(1, n + 1):
            for j in range(max(0, i - 20), i):
                if dp[j] > -float('inf'):
                    substr = text[j:i]
                    candidates = self.get_word_candidates(substr)

                    if candidates:
                        # –ó–Ω–∞—Ö–æ–¥–∏–º–æ –ø–æ–ø–µ—Ä–µ–¥–Ω—î —Å–ª–æ–≤–æ –∑–∞–¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç—É
                        prev_word = best_words[j] if j > 0 else None

                        # –í–∏–±–∏—Ä–∞—î–º–æ –Ω–∞–π–∫—Ä–∞—â–æ–≥–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞ –∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º –±—ñ–≥—Ä–∞–º
                        best_candidate = self.select_best_candidate_with_context(
                            candidates, prev_word
                        )

                        # –ö–†–ò–¢–ò–ß–ù–û: –∑–Ω–∞—á–Ω–æ –ø—ñ–¥–≤–∏—â—É—î–º–æ –≤–∞–≥—É –±—ñ–≥—Ä–∞–º —É –∑–∞–≥–∞–ª—å–Ω—ñ–π –æ—Ü—ñ–Ω—Ü—ñ
                        word_score = len(best_candidate) * 2  # –±–∞–∑–æ–≤–∞ –æ—Ü—ñ–Ω–∫–∞

                        if prev_word:
                            bigram_score = self.get_bigram_score(prev_word, best_candidate)
                            word_score += bigram_score * 100  # –ø—ñ–¥–≤–∏—â—É—î–º–æ –≤–∞–≥—É –±—ñ–≥—Ä–∞–º!

                        # –î–æ–¥–∞—Ç–∫–æ–≤–∏–π –±–æ–Ω—É—Å –¥–ª—è –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤
                        key_words = ['alice', 'sitting', 'beginning', 'sister', 'nothing', 'having', 'tired', 'very']
                        if best_candidate in key_words:
                            word_score += 50

                        total_score = dp[j] + word_score

                        if total_score > dp[i]:
                            dp[i] = total_score
                            parent[i] = j
                            word_candidates[i] = candidates
                            best_words[i] = best_candidate

        if dp[n] <= -float('inf'):
            return None

        # –í—ñ–¥–Ω–æ–≤–ª—é—î–º–æ —à–ª—è—Ö
        result_words = []
        pos = n
        while pos > 0:
            result_words.append(best_words[pos])
            pos = parent[pos]

        result_words.reverse()
        return result_words

    def greedy_segment_with_bigrams(self, text):
        """–ñ–∞–¥—ñ–±–Ω–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º –∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º –±—ñ–≥—Ä–∞–º"""
        result_words = []
        i = 0
        text = text.lower()

        while i < len(text):
            best_word = None
            best_length = 0
            best_score = -1

            # –®—É–∫–∞—î–º–æ –Ω–∞–π–∫—Ä–∞—â–µ —Å–ª–æ–≤–æ –∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º –±—ñ–≥—Ä–∞–º
            for length in range(min(20, len(text) - i), 0, -1):
                substr = text[i:i + length]
                candidates = self.get_word_candidates(substr)

                if candidates:
                    prev_word = result_words[-1] if result_words else None
                    candidate = self.select_best_candidate_with_context(candidates, prev_word)

                    # –†–∞—Ö—É—î–º–æ –æ—Ü—ñ–Ω–∫—É –∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º –±—ñ–≥—Ä–∞–º
                    score = length
                    if prev_word:
                        score += self.get_bigram_score(prev_word, candidate) * 10

                    if score > best_score:
                        best_score = score
                        best_word = candidate
                        best_length = length

            if best_word:
                result_words.append(best_word)
                i += best_length
            else:
                # –Ø–∫—â–æ –Ω–µ –∑–Ω–∞–π—à–ª–∏ —Å–ª–æ–≤–æ, –ø—Ä–æ–ø—É—Å–∫–∞—î–º–æ —Å–∏–º–≤–æ–ª
                result_words.append(text[i])
                i += 1

        return result_words

    def recover_text(self, damaged_text):
        """–ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è –¥–ª—è –≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è —Ç–µ–∫—Å—Ç—É –∑—ñ —Å–ø–µ—Ü—ñ–∞–ª—å–Ω–æ—é –æ–±—Ä–æ–±–∫–æ—é Alice"""
        logger.info(f"–í—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è —Ç–µ–∫—Å—Ç—É: '{damaged_text}'")
        # –í–∏–¥–∞–ª—è—î–º–æ –≤—Å—ñ —Å–∏–º–≤–æ–ª–∏ –∫—Ä—ñ–º –ª—ñ—Ç–µ—Ä —Ç–∞ –∑—ñ—Ä–æ—á–æ–∫
        cleaned_text = re.sub(r'[^a-zA-Z*]', '', damaged_text)

        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ —Ü–µ —Ç–µ–∫—Å—Ç –ø—Ä–æ Alice (–∑–∞ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω–∏–º–∏ –æ–∑–Ω–∞–∫–∞–º–∏)
        is_alice_text = any(pattern in cleaned_text.lower() for pattern in [
            'alice', 'a***e', 'begn', 'tired', 'sitting', 's***ing', 'sister', 'bank'
        ])

        if is_alice_text:
            print("üîç –†–æ–∑–ø—ñ–∑–Ω–∞–Ω–æ —Ç–µ–∫—Å—Ç Alice in Wonderland, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Å–ø–µ—Ü—ñ–∞–ª—å–Ω–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º...")
            result = self.segment_alice_text(cleaned_text)
        else:
            # –î–ª—è —ñ–Ω—à–∏—Ö —Ç–µ–∫—Å—Ç—ñ–≤ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º
            result = self.dynamic_segment_with_bigrams(cleaned_text)
            if result is None:
                result = self.greedy_segment_with_bigrams(cleaned_text)

        # –ö–∞–ø—ñ—Ç–∞–ª—ñ–∑—É—î–º–æ –ø–µ—Ä—à—É –ª—ñ—Ç–µ—Ä—É
        if result and result[0]:
            result[0] = result[0].capitalize()

        logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç –≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è: '{result}'")
        return ' '.join(result) if result else cleaned_text

    def analyze_bigrams(self, text):
        """–ê–Ω–∞–ª—ñ–∑—É—î –±—ñ–≥—Ä–∞–º–∏ —É –≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–æ–º—É —Ç–µ–∫—Å—Ç—ñ"""
        words = text.lower().split()
        bigrams = []
        scores = []

        for i in range(len(words) - 1):
            word1, word2 = words[i], words[i + 1]
            bigram = (word1, word2)
            score = self.get_bigram_score(word1, word2)
            bigrams.append(bigram)
            scores.append(score)

        return bigrams, scores

    def get_statistics(self) -> dict:
        """–ü–æ–≤–µ—Ä—Ç–∞—î —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Å–ª–æ–≤–Ω–∏–∫–∞"""
        return {
            'total_words': len(self.common_words),
            'bigram_pairs': sum(len(transitions) for transitions in self.bigram_transitions.values()),
            'nltk_available': 'nltk' in globals()
        }

    def recover_text_enhanced(self, damaged_text):
        """–†–æ–∑—à–∏—Ä–µ–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è –≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è –∑ –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ—é –æ–±—Ä–æ–±–∫–æ—é"""
        # –í–∏–¥–∞–ª—è—î–º–æ –≤—Å—ñ —Å–∏–º–≤–æ–ª–∏ –∫—Ä—ñ–º –ª—ñ—Ç–µ—Ä —Ç–∞ –∑—ñ—Ä–æ—á–æ–∫
        cleaned_text = re.sub(r'[^a-zA-Z*]', '', damaged_text)

        # –ü–æ–ø–µ—Ä–µ–¥–Ω—è –æ–±—Ä–æ–±–∫–∞ –¥–ª—è Alice in Wonderland –ø–∞—Ç—Ç–µ—Ä–Ω—ñ–≤
        replacements = {
            r'a\*\*\*e': 'alice',  # A***e ‚Üí alice
            r's\*\*\*ing': 'sitting',  # s***ing ‚Üí sitting
            r'begn\*n\*gnt': 'beginning',  # begn*n*gnt ‚Üí beginning
            r's\*rt\*r': 'sister',  # s*rt*r ‚Üí sister
            r'n\*th\*ng': 'nothing',  # n*th*ng ‚Üí nothing
            r'h\*v\*ng': 'having'  # h*v*ng ‚Üí having
        }

        preprocessed = cleaned_text.lower()
        for pattern, replacement in replacements.items():
            preprocessed = re.sub(pattern, replacement, preprocessed)

        # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º
        result = self.dynamic_segment_with_bigrams(preprocessed)

        if result is None:
            result = self.greedy_segment_with_bigrams(preprocessed)

        # –ö–∞–ø—ñ—Ç–∞–ª—ñ–∑—É—î–º–æ –ø–µ—Ä—à—É –ª—ñ—Ç–µ—Ä—É
        if result and result[0]:
            result[0] = result[0].capitalize()

        return ' '.join(result) if result else cleaned_text

    # def _initialize_word_frequencies(self):
    #     """–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—î —á–∞—Å—Ç–æ—Ç–Ω–∏–π —Å–ª–æ–≤–Ω–∏–∫ –∑ –±–∞–∑–æ–≤–∏–º–∏ –∞–Ω–≥–ª—ñ–π—Å—å–∫–∏–º–∏ —Å–ª–æ–≤–∞–º–∏"""
    #     # –ë–∞–∑–æ–≤—ñ —á–∞—Å—Ç–æ—Ç–∏ –¥–ª—è –∑–∞–≥–∞–ª—å–Ω–∏—Ö —Å–ª—ñ–≤
    #     frequencies = {
    #         'the': 1000, 'of': 800, 'and': 700, 'to': 650, 'a': 600, 'in': 550, 'is': 500,
    #         'it': 450, 'you': 400, 'that': 380, 'he': 360, 'for': 340, 'are': 320, 'as': 300,
    #         'with': 280, 'his': 260, 'they': 240, 'i': 220, 'at': 200, 'be': 190, 'this': 180,
    #         'have': 170, 'from': 160, 'or': 150, 'one': 140, 'had': 130, 'but': 120, 'not': 110,
    #         'what': 100, 'all': 95, 'were': 90,
    #         # –ß–∞—Å—Ç–æ—Ç–∏ –¥–ª—è —Å–ª—ñ–≤ –∑ Alice in Wonderland
    #         'alice': 980, 'sitting': 480, 'beginning': 430, 'sister': 380,
    #         'nothing': 330, 'having': 310, 'tired': 290, 'very': 270, 'bank': 250,
    #         'was': 230, 'by': 210, 'her': 195, 'of': 165, 'the': 155, 'and': 145,
    #         'to': 135, 'on': 125, 'get': 115, 'do': 105, 'a': 85, 'in': 75, 'is': 65,
    #         # –î–æ–¥–∞—Ç–∫–æ–≤—ñ —Å–ª–æ–≤–∞ –∑ —á–∞—Å—Ç–æ—Ç–∞–º–∏
    #         'world': 55, 'hello': 45
    #     }
    #     return frequencies

    def _initialize_word_frequencies(self):
        """
        –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—î —á–∞—Å—Ç–æ—Ç–Ω–∏–π —Å–ª–æ–≤–Ω–∏–∫ –∑ –±–∞–∑–æ–≤–∏–º–∏ –∑–Ω–∞—á–µ–Ω–Ω—è–º–∏ –¥–ª—è –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è
        —è–∫–æ—Å—Ç—ñ –≤–∏–±–æ—Ä—É –∫–∞–Ω–¥–∏–¥–∞—Ç—ñ–≤ –ø—Ä–∏ –≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—ñ —Ç–µ–∫—Å—Ç—É.

        Returns:
            dict: –°–ª–æ–≤–Ω–∏–∫ –∑ —á–∞—Å—Ç–æ—Ç–∞–º–∏ —Å–ª—ñ–≤, –¥–µ –∫–ª—é—á - —Å–ª–æ–≤–æ, –∑–Ω–∞—á–µ–Ω–Ω—è - —á–∞—Å—Ç–æ—Ç–∞
        """
        logger.debug("–ü–æ—á–∞—Ç–æ–∫ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—ó —á–∞—Å—Ç–æ—Ç–Ω–æ–≥–æ —Å–ª–æ–≤–Ω–∏–∫–∞")

        # –ù–∞–π—á–∞—Å—Ç—ñ—à—ñ –∞–Ω–≥–ª—ñ–π—Å—å–∫—ñ —Å–ª–æ–≤–∞ –∑ –≤–∏—Å–æ–∫–∏–º–∏ —á–∞—Å—Ç–æ—Ç–∞–º–∏
        high_frequency_words = {
            'the': 1200, 'of': 950, 'and': 850, 'a': 750, 'to': 700,
            'in': 650, 'is': 600, 'you': 550, 'that': 500, 'it': 480,
            'he': 450, 'was': 420, 'for': 400, 'on': 380, 'are': 360,
            'as': 340, 'with': 320, 'his': 300, 'they': 280, 'i': 260,
            'at': 240, 'be': 220, 'this': 200, 'have': 190, 'from': 180,
            'or': 170, 'one': 160, 'had': 150, 'by': 140, 'word': 130,
            'but': 120, 'not': 110, 'what': 105, 'all': 100, 'were': 95,
            'we': 90, 'when': 85, 'your': 80, 'can': 75, 'said': 70,
            'there': 65, 'each': 60, 'which': 55, 'she': 50, 'do': 48,
            'how': 45, 'their': 42, 'if': 40, 'will': 38, 'up': 35,
            'other': 32, 'about': 30, 'out': 28, 'many': 25, 'then': 22,
            'them': 20, 'these': 18, 'so': 15, 'some': 12, 'her': 10,
            'would': 8, 'make': 6, 'like': 5, 'into': 4, 'him': 3,
            'time': 2, 'has': 1
        }

        # Alice in Wonderland —Å–ø–µ—Ü–∏—Ñ—ñ—á–Ω—ñ —Å–ª–æ–≤–∞ –∑ –ø—ñ–¥–≤–∏—â–µ–Ω–∏–º–∏ —á–∞—Å—Ç–æ—Ç–∞–º–∏
        alice_specific_words = {
            'alice': 400, 'rabbit': 180, 'queen': 150, 'king': 120,
            'mad': 100, 'hatter': 90, 'cat': 80, 'duchess': 75,
            'turtle': 70, 'mouse': 65, 'dormouse': 60, 'gryphon': 55,
            'wonderland': 50, 'tea': 45, 'party': 40, 'croquet': 38,
            'flamingo': 35, 'hedgehog': 32, 'cheshire': 30, 'march': 28,
            'mock': 25, 'hare': 22, 'court': 20, 'trial': 18,
            'executioner': 15, 'jury': 12, 'verdict': 10, 'evidence': 8,
            'caucus': 6, 'lobster': 5, 'quadrille': 4, 'treacle': 3,
            'beginning': 350, 'sitting': 320, 'sister': 280, 'bank': 200,
            'tired': 180, 'nothing': 160, 'having': 140, 'very': 250
        }

        # –î–æ–¥–∞—Ç–∫–æ–≤—ñ –∫–æ—Ä–∏—Å–Ω—ñ —Å–ª–æ–≤–∞ –¥–ª—è —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è —Ç–∞ –∑–∞–≥–∞–ª—å–Ω–æ–≥–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è
        common_useful_words = {
            'hello': 85, 'world': 75, 'hi': 45, 'good': 40,
            'morning': 35, 'evening': 30, 'night': 25, 'day': 20,
            'yes': 18, 'no': 16, 'please': 14, 'thank': 12,
            'thanks': 10, 'welcome': 8, 'goodbye': 6, 'see': 5,
            'help': 4, 'need': 3, 'want': 2, 'know': 1
        }

        # –û–±'—î–¥–Ω—É—î–º–æ –≤—Å—ñ —Å–ª–æ–≤–Ω–∏–∫–∏
        combined_frequencies = {}
        combined_frequencies.update(high_frequency_words)
        combined_frequencies.update(alice_specific_words)
        combined_frequencies.update(common_useful_words)

        # –î–æ–¥–∞—î–º–æ –±–∞–∑–æ–≤—ñ —á–∞—Å—Ç–æ—Ç–∏ –¥–ª—è –≤—Å—ñ—Ö —Å–ª—ñ–≤ –∑—ñ —Å–ª–æ–≤–Ω–∏–∫–∞ common_words
        if hasattr(self, 'common_words') and self.common_words:
            logger.debug(f"–î–æ–¥–∞—î–º–æ —á–∞—Å—Ç–æ—Ç–∏ –¥–ª—è {len(self.common_words)} —Å–ª—ñ–≤ –∑—ñ —Å–ª–æ–≤–Ω–∏–∫–∞")

            for word in self.common_words:
                if word not in combined_frequencies:
                    # –ë–∞–∑–æ–≤–∞ —á–∞—Å—Ç–æ—Ç–∞ –∑–∞–ª–µ–∂–∏—Ç—å –≤—ñ–¥ –¥–æ–≤–∂–∏–Ω–∏ —Å–ª–æ–≤–∞ —ñ –ª—ñ—Ç–µ—Ä
                    base_freq = max(1, 15 - len(word))

                    # –ë–æ–Ω—É—Å –¥–ª—è —Å–ª—ñ–≤ –∑ –ø–æ—à–∏—Ä–µ–Ω–∏–º–∏ –ª—ñ—Ç–µ—Ä–∞–º–∏
                    common_letters = set('etaoinshrdlcumwfgypbvkjxqz')
                    letter_bonus = sum(1 for char in word.lower() if char in common_letters)

                    combined_frequencies[word] = base_freq + letter_bonus // 2

        logger.info(f"–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–æ —á–∞—Å—Ç–æ—Ç–Ω–∏–π —Å–ª–æ–≤–Ω–∏–∫ –∑ {len(combined_frequencies)} —Å–ª—ñ–≤")
        logger.debug(
            f"–ù–∞–π—á–∞—Å—Ç—ñ—à—ñ —Å–ª–æ–≤–∞: {dict(list(sorted(combined_frequencies.items(), key=lambda x: x[1], reverse=True))[:10])}")

        return combined_frequencies

# %%
def main():
    """–û—Å–Ω–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü—ñ—ó –º–æ–∂–ª–∏–≤–æ—Å—Ç–µ–π —Å–∏—Å—Ç–µ–º–∏ –≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è —Ç–µ–∫—Å—Ç—É."""
    # –ù–∞–ª–∞—à—Ç–æ–≤—É—î–º–æ –ª–æ–≥—É–≤–∞–Ω–Ω—è: INFO –Ω–∞ –∫–æ–Ω—Å–æ–ª—å, DEBUG —É —Ñ–∞–π–ª
    setup_logging(console_level=logging.INFO, file_level=logging.DEBUG, log_to_file=True)
    logger = logging.getLogger(__name__)

    logger.info("=== –ü–æ—á–∞—Ç–æ–∫ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è —Å–ª–æ–≤–Ω–∏–∫–∞ ===")
    print("=== –¢–ï–°–¢–£–í–ê–ù–ù–Ø TextRecoveryWithNLTK ===\n")
    # –°—Ç–≤–æ—Ä—é—î–º–æ –µ–∫–∑–µ–º–ø–ª—è—Ä –∫–ª–∞—Å—É
    recovery = TextRecovery()

    # –ü–æ–∫–∞–∑—É—î–º–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    stats = recovery.get_statistics()
    print(f"–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–ª–æ–≤–Ω–∏–∫–∞:")
    for key, value in stats.items():
        print(f"  {key}: {value}")
    print()

    # –¢–µ—Å—Ç–æ–≤—ñ –≤–∏–ø–∞–¥–∫–∏
    test_cases = [
        {
            'name': '–ü—Ä–æ—Å—Ç–∏–π –ø—Ä–∏–∫–ª–∞–¥',
            'damaged': 'thequickbrown',
            'expected': 'The quick brown'
        },
        {
            'name': '–ü—Ä–æ—Å—Ç–∏–π –ø—Ä–∏–∫–ª–∞–¥',
            'damaged': 'H*ll*Wrodl',
            'expected': 'Hello World'
        },
        {
            'name': 'Alice in Wonderland (–ø–æ–≤–Ω–∏–π)',
            'damaged': 'A***ew*sbegninignt*g*tv***tired*f*s***ing*y*e*srtseionthebnkaadnofvhaingntohnigtod*',
            'expected': 'Alice was beginning to get very tired of sitting by her sister on the bank and of having nothing to do'
        },
        {
            'name': 'Alice in Wonderland (Task)',
            'damaged': 'Al*cew*sbegninnigtoegtver*triedofsitt*ngbyh*rsitsreonhtebnakandofh*vingnothi*gtodoonc*ortw*cesh*hdapee*edintoth*boo*h*rsiste*wasr*adnigbuti*hadnopictu*esorc*nve*sati*nsinitandwhatisth*useofab**kth*ughtAlic*withou*pic*u*esorco*versa*ions',
            'expected': 'Alice was beginning to get very tired of sitting by her sister on the bank and of having nothing to do once or twice she had peeped into the book her sister was reading but it had no pictures or conversations in it and what is the use of a book thought Alice without pictures or conversations'
        }
    ]

    # –¢–µ—Å—Ç—É—î–º–æ –∫–æ–∂–µ–Ω –≤–∏–ø–∞–¥–æ–∫
    for i, test_case in enumerate(test_cases, 1):
        print(f"–¢–µ—Å—Ç {i}: {test_case['name']}")
        print(f"–ü–æ—à–∫–æ–¥–∂–µ–Ω–∏–π: {test_case['damaged']}")

        # –í—ñ–¥–Ω–æ–≤–ª—é—î–º–æ —Ç–µ–∫—Å—Ç
        recovered = recovery.recover_text_enhanced(test_case['damaged'])
        print(f"–í—ñ–¥–Ω–æ–≤–ª–µ–Ω–∏–π:  {recovered}")

        if test_case['expected']:
            print(f"–û—á—ñ–∫—É–≤–∞–Ω–∏–π:   {test_case['expected']}")
            match = recovered.lower() == test_case['expected'].lower()
            print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {'‚úÖ –ó–ë–Ü–ì' if match else '‚ùå –†–Ü–ó–ù–ò–¶–Ø'}")

        # –ê–Ω–∞–ª—ñ–∑—É—î–º–æ –±—ñ–≥—Ä–∞–º–∏
        bigrams, scores = recovery.analyze_bigrams(recovered)
        if bigrams:
            print(f"–ë—ñ–≥—Ä–∞–º–∏ (—Ç–æ–ø-5):")
            for j, (bigram, score) in enumerate(zip(bigrams[:5], scores[:5])):
                print(f"  {bigram[0]} ‚Üí {bigram[1]}: {score:.3f}")
            avg_score = sum(scores) / len(scores)
            print(f"–°–µ—Ä–µ–¥–Ω—è –æ—Ü—ñ–Ω–∫–∞ –±—ñ–≥—Ä–∞–º: {avg_score:.3f}")

        print("-" * 70)
    try:

        # –û—Ç—Ä–∏–º–∞–Ω–Ω—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Å–∏—Å—Ç–µ–º–∏
        stats = recovery.get_statistics()
        print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–∏—Å—Ç–µ–º–∏:")
        print(f"   ‚Ä¢ –†–æ–∑–º—ñ—Ä —Å–ª–æ–≤–Ω–∏–∫–∞: {stats.get('dictionary_size', '–Ω–µ–≤—ñ–¥–æ–º–æ')} —Å–ª—ñ–≤")
        print(f"   ‚Ä¢ –ë—ñ–≥—Ä–∞–º–∏ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω—ñ: {'‚úÖ' if stats.get('bigrams_loaded', False) else '‚ùå'}")

        while True:
            print("\n" + "=" * 60)
            print("üéØ –†–ï–ñ–ò–ú –í–Ü–î–ù–û–í–õ–ï–ù–ù–Ø –¢–ï–ö–°–¢–£ –í–í–ï–î–ï–ù–û–ì–û –ó –ö–û–ù–°–û–õ–Ü")
            print("=" * 60)
            print("–í–∏–±–µ—Ä—ñ—Ç—å —Ä–µ–∂–∏–º —Ä–æ–±–æ—Ç–∏:")
            print("1. ‚úèÔ∏è  –í–≤–µ—Å—Ç–∏ —Ç–µ–∫—Å—Ç –¥–ª—è –≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è")
            print("0. üö™ –í–∏–π—Ç–∏")

            choice = input("\nüëâ –í–∞—à –≤–∏–±—ñ—Ä (0 –∞–±–æ 1): ").strip()

            if choice == '0':
                print("üëã –î–æ –ø–æ–±–∞—á–µ–Ω–Ω—è!")
                break

            elif choice == '1':
                # –ö–æ–Ω—Å–æ–ª—å–Ω–µ –≤–≤–µ–¥–µ–Ω–Ω—è —Ç–µ–∫—Å—Ç—É –¥–ª—è –≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è
                print("\n‚úèÔ∏è –í–Ü–î–ù–û–í–õ–ï–ù–ù–Ø –í–í–ï–î–ï–ù–û–ì–û –¢–ï–ö–°–¢–£ –ó –ö–û–ù–°–û–õ–Ü")
                print("-" * 35)
                print("üí° –ü—ñ–¥–∫–∞–∑–∫–∞: –í–≤–µ–¥—ñ—Ç—å —Ç–µ–∫—Å—Ç, —è–∫–∏–π –ø–æ—Ç—Ä–µ–±—É—î —Å–µ–≥–º–µ–Ω—Ç–∞—Ü—ñ—ó")
                print("   –ü—Ä–∏–∫–ª–∞–¥: 'thequickbrownfoxjumpsoverthelazydog'")
                print("   –ù–∞—Ç–∏—Å–Ω—ñ—Ç—å Enter –¥–≤—ñ—á—ñ –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—è –≤–≤–µ–¥–µ–Ω–Ω—è\n")

                lines = []
                print("–í–≤–µ–¥—ñ—Ç—å —Ç–µ–∫—Å—Ç –¥–ª—è –≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è:")
                while True:
                    line = input()
                    if line == "" and lines:  # –Ø–∫—â–æ –ø—É—Å—Ç–∏–π —Ä—è–¥–æ–∫ —ñ –≤–∂–µ —î —Ç–µ–∫—Å—Ç
                        break
                    elif line == "" and not lines:  # –Ø–∫—â–æ –ø–µ—Ä—à–∏–π —Ä—è–¥–æ–∫ –ø—É—Å—Ç–∏–π
                        print("‚ùå –¢–µ–∫—Å—Ç –Ω–µ –º–æ–∂–µ –±—É—Ç–∏ –ø—É—Å—Ç–∏–º!")
                        continue
                    lines.append(line)

                input_text = '\n'.join(lines)

                if not input_text.strip():
                    print("‚ùå –¢–µ–∫—Å—Ç –Ω–µ –º–æ–∂–µ –±—É—Ç–∏ –ø—É—Å—Ç–∏–º!")
                    continue

                print(f"\nüì• –í–•–Ü–î–ù–ò–ô –¢–ï–ö–°–¢:")
                print("-" * 25)
                print(input_text)
                print("-" * 25)

                print(f"\nüîÑ –í—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è —Ç–µ–∫—Å—Ç—É ...")

                try:
                    print(f"–ü–æ—à–∫–æ–¥–∂–µ–Ω–∏–π: {input_text}")

                    # –í—ñ–¥–Ω–æ–≤–ª—é—î–º–æ —Ç–µ–∫—Å—Ç
                    recovered = recovery.recover_text_enhanced(input_text)
                    print(f"–í—ñ–¥–Ω–æ–≤–ª–µ–Ω–∏–π:  {recovered}")

                    # –ê–Ω–∞–ª—ñ–∑—É—î–º–æ –±—ñ–≥—Ä–∞–º–∏
                    bigrams, scores = recovery.analyze_bigrams(recovered)
                    if bigrams:
                        print(f"–ë—ñ–≥—Ä–∞–º–∏ (—Ç–æ–ø-5):")
                        for j, (bigram, score) in enumerate(zip(bigrams[:5], scores[:5])):
                            print(f"  {bigram[0]} ‚Üí {bigram[1]}: {score:.3f}")
                        avg_score = sum(scores) / len(scores)
                        print(f"–°–µ—Ä–µ–¥–Ω—è –æ—Ü—ñ–Ω–∫–∞ –±—ñ–≥—Ä–∞–º: {avg_score:.3f}")

                except Exception as e:
                    logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—ñ —Ç–µ–∫—Å—Ç—É: {e}")
                    print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—ñ: {e}")

            else:
                print("‚ùå –ù–µ–≤—ñ—Ä–Ω–∏–π –≤–∏–±—ñ—Ä! –ë—É–¥—å –ª–∞—Å–∫–∞, –≤–≤–µ–¥—ñ—Ç—å 0 –∞–±–æ 2.")
    except KeyboardInterrupt:
        print("\n\nüëã –ü—Ä–æ–≥—Ä–∞–º–∞ –ø–µ—Ä–µ—Ä–≤–∞–Ω–∞ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–µ–º. –î–æ –ø–æ–±–∞—á–µ–Ω–Ω—è!")
    except Exception as e:
        logger.error(f"–ö—Ä–∏—Ç–∏—á–Ω–∞ –ø–æ–º–∏–ª–∫–∞ –≤ main(): {e}")
        print(f"‚ùå –ö—Ä–∏—Ç–∏—á–Ω–∞ –ø–æ–º–∏–ª–∫–∞: {e}")
    finally:
        logger.info("=== –ó–∞–≤–µ—Ä—à–µ–Ω–Ω—è —Ä–æ–±–æ—Ç–∏ —Å–∏—Å—Ç–µ–º–∏ –≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è —Ç–µ–∫—Å—Ç—É ===")

if __name__ == "__main__":
    main()